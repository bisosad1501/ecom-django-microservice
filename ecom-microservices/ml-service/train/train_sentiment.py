import os
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Input, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import joblib
import pickle
import re
import nltk
from nltk.corpus import stopwords

# ğŸ“ Cáº¥u hÃ¬nh thÆ° má»¥c
MODEL_DIR = "ml-service/train/models"
os.makedirs(MODEL_DIR, exist_ok=True)

# ğŸ“¥ Táº£i NLTK resources
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# ğŸ§¹ HÃ m tiá»n xá»­ lÃ½ vÄƒn báº£n
def preprocess_text(text):
    if isinstance(text, str):
        # Chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng
        text = text.lower()
        # Loáº¡i bá» HTML tags
        text = re.sub(r'<.*?>', '', text)
        # Loáº¡i bá» cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t vÃ  sá»‘
        text = re.sub(r'[^\w\s]', '', text)
        text = re.sub(r'\d+', '', text)
        # Loáº¡i bá» stopwords
        stop_words = set(stopwords.words('english'))
        text = ' '.join([word for word in text.split() if word not in stop_words])
        return text
    return ""

# ğŸ“Š HÃ m táº¡o biá»ƒu Ä‘á»“ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh
def plot_training_history(history, filename="model_history.png"):
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='lower right')
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper right')
    
    plt.tight_layout()
    plt.savefig(os.path.join(MODEL_DIR, filename))
    plt.close()

# ğŸ“¥ Load dá»¯ liá»‡u
print("ğŸ”„ Loading dataset...")
DATA_PATH = "ml-service/train/datasets/sentiment_train.csv"
df = pd.read_csv(DATA_PATH)

# ğŸ”„ Tiá»n xá»­ lÃ½ dá»¯ liá»‡u
print("ğŸ§¹ Preprocessing text data...")
df['processed_review'] = df['review'].apply(preprocess_text)

# Chuyá»ƒn nhÃ£n thÃ nh sá»‘
sentiment_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}
df['sentiment_label'] = df['sentiment'].map(sentiment_mapping)

# âœ‚ï¸ Chia dá»¯ liá»‡u thÃ nh train & validation & test
X = df['processed_review']
y = df['sentiment_label']
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# ğŸ”  Tokenization vÃ  Padding
print("ğŸ”  Tokenizing text...")
MAX_WORDS = 10000  # Sá»‘ lÆ°á»£ng tá»« tá»‘i Ä‘a trong tá»« Ä‘iá»ƒn
MAX_SEQUENCE_LENGTH = 200  # Äá»™ dÃ i tá»‘i Ä‘a cá»§a má»—i vÄƒn báº£n

tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)

# LÆ°u tokenizer
with open(os.path.join(MODEL_DIR, 'tokenizer.pickle'), 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

# Chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i sá»‘
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_val_seq = tokenizer.texts_to_sequences(X_val)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Padding chuá»—i
X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post')
X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post')

# ğŸ— XÃ¢y dá»±ng mÃ´ hÃ¬nh LSTM
print("ğŸ— Building model...")
EMBEDDING_DIM = 128  # KÃ­ch thÆ°á»›c vector nhÃºng
vocab_size = len(tokenizer.word_index) + 1  # KÃ­ch thÆ°á»›c tá»« Ä‘iá»ƒn

# Äá»‹nh nghÄ©a kiáº¿n trÃºc mÃ´ hÃ¬nh
model = Sequential()
model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))
model.add(SpatialDropout1D(0.2))
model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(3, activation='softmax'))  # 3 classes: negative, neutral, positive

# BiÃªn dá»‹ch mÃ´ hÃ¬nh
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# TÃ³m táº¯t thÃ´ng tin mÃ´ hÃ¬nh
model.summary()

# ğŸ‹ï¸â€â™‚ï¸ Huáº¥n luyá»‡n mÃ´ hÃ¬nh
print("ğŸ‹ï¸â€â™‚ï¸ Training model...")
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
checkpoint = ModelCheckpoint(
    os.path.join(MODEL_DIR, 'best_model.h5'),
    monitor='val_accuracy',
    save_best_only=True,
    mode='max'
)

history = model.fit(
    X_train_pad, y_train,
    validation_data=(X_val_pad, y_val),
    epochs=10,
    batch_size=64,
    callbacks=[early_stopping, checkpoint]
)

# ğŸ“Š Váº½ biá»ƒu Ä‘á»“ káº¿t quáº£ huáº¥n luyá»‡n
plot_training_history(history)

# ğŸ¯ ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p test
print("ğŸ¯ Evaluating model...")
y_pred_proba = model.predict(X_test_pad)
y_pred = np.argmax(y_pred_proba, axis=1)

print("ğŸ“Š Classification Report:")
print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))

# TÃ­nh toÃ¡n Ä‘á»™ chÃ­nh xÃ¡c
accuracy = accuracy_score(y_test, y_pred)
print(f"âœ… Accuracy: {accuracy:.4f}")

# Táº¡o ma tráº­n nháº§m láº«n
cm = confusion_matrix(y_test, y_pred)
print("ğŸ“Š Confusion Matrix:")
print(cm)

# ğŸ’¾ LÆ°u mÃ´ hÃ¬nh
print("ğŸ’¾ Saving model...")
model.save(os.path.join(MODEL_DIR, 'sentiment_lstm.h5'))

# LÆ°u mapping nhÃ£n
joblib.dump(sentiment_mapping, os.path.join(MODEL_DIR, 'sentiment_mapping.pkl'))

print("âœ… Training complete!")
print(f"ğŸ“ Model saved to: {os.path.join(MODEL_DIR, 'sentiment_lstm.h5')}")